{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helvete74/IFAGE_dev_AI/blob/main/projet_head_detection_V1_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CJQVJ8qEBtG"
      },
      "source": [
        "Head Detection V1.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSy63KfET-3"
      },
      "source": [
        "https://universe.roboflow.com/head-znpny/head-detection-nej1a/\n",
        "\n",
        "charge le dataset au YoloV8\n",
        "model pre-entrainé : yolo_v8_XXX\n",
        "\n",
        "différents essais pour améliorer la detection/ supprimer les doubles detections\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNxY85JFJDXW"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjrPnHkqJFns"
      },
      "outputs": [],
      "source": [
        "YOLOV8BACKBONE = \"yolo_v8_s_backbone\"\n",
        "# YOLOV8BACKBONE = \"yolo_v8_s_backbone_coco\"\n",
        "# YOLOV8BACKBONE = \"yolo_v8_m_backbone_coco\"\n",
        "# YOLOV8BACKBONE = \"yolo_v8_l_backbone\"\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCH = 70\n",
        "GLOBAL_CLIPNORM = 10.0\n",
        "\n",
        "BOUNDING_BOX_FORMAT = \"xyxy\"\n",
        "\n",
        "IOU_THRESHOLD = 0.5\n",
        "CONFIDENCE_THRESHOLD = 0.75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7DYcTHtEvBX"
      },
      "source": [
        "#Installation et import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RKpBMOkHIc6G"
      },
      "outputs": [],
      "source": [
        "!pip install -U wandb\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "\n",
        "wandb.login(key=\"ea33b1bdf2e44dad5cdbc06372974ee5207fd710\")\n",
        "# API key : ea33b1bdf2e44dad5cdbc06372974ee5207fd710"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tj4QjrDGE1GV"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "!pip list| grep roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "no_j7gmeFGIk"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+https://github.com/keras-team/keras-cv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmZA8ATdFZRv"
      },
      "outputs": [],
      "source": [
        "# check install keras\n",
        "!pip list| grep keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uLpAicYFdja"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import keras_cv\n",
        "from keras_cv import bounding_box\n",
        "from keras_cv import visualization\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Gihuwgcbph"
      },
      "source": [
        "# Creation du Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk1eXiHzEnqY"
      },
      "source": [
        "## Téléchargement depuis roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B7tX_aJDyX8"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"4RTem5JDv2kRxw8jk1El\")\n",
        "project = rf.workspace(\"head-znpny\").project(\"head-detection-nej1a\")\n",
        "version = project.version(1)\n",
        "roboflow_dataset = version.download(\"voc\")\n",
        "\n",
        "\n",
        "\n",
        "model = version.model\n",
        "str(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwNEyAb4aWJv"
      },
      "outputs": [],
      "source": [
        "roboflow_dataset.location\n",
        "roboflow_dataset.model_format\n",
        "roboflow_dataset.name\n",
        "roboflow_dataset.version\n",
        "\n",
        "\n",
        "print(roboflow_dataset.location)\n",
        "data_path = roboflow_dataset.location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6mBb7keMAOH"
      },
      "source": [
        "en format voc, structure des fichiers :\\\n",
        "head-detection-1\n",
        "*   test\n",
        "  *   file.jpg\n",
        "  *   file.xml\n",
        "*   train\n",
        "  *   file.jpg\n",
        "  *   file.xml\n",
        "*   valid\n",
        "  *   file.jpg\n",
        "  *   file.xml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqr-dc6ZvyhS"
      },
      "source": [
        "## Manipulation de fichier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd9UJxX8v1HS"
      },
      "outputs": [],
      "source": [
        "# suppression de sample_data\n",
        "!rm -rfv sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfy8wyercTXK"
      },
      "source": [
        "## Manipulation des datas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UM036KwJKWT"
      },
      "source": [
        "A dictionary is created to map each class name to a unique numerical identifier. This mapping is used to encode and decode the class labels during training and inference in object detection tasks.\\\n",
        "\n",
        "The"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6aWFQvKJLDo"
      },
      "outputs": [],
      "source": [
        "# class_ids from the Udacity Self Driving Car Dataset\n",
        "class_ids = [\n",
        "    \"0\",\n",
        "    \"null\"\n",
        "]\n",
        "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
        "class_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y67hvIzSKkfj"
      },
      "outputs": [],
      "source": [
        "# Path to images and annotations\n",
        "# path_images = \"/kaggle/input/dataset/data/images/\"\n",
        "# path_annot = \"/kaggle/input/dataset/data/annotations/\"\n",
        "# data_path = /content/head-detection-1\n",
        "\n",
        "\n",
        "def parse_directory(path_data):\n",
        "  # Get all XML file paths in path_annot and sort them\n",
        "  xml_files = sorted(\n",
        "      [\n",
        "          os.path.join(path_data, file_name)\n",
        "          for file_name in os.listdir(path_data)\n",
        "          if file_name.endswith(\".xml\")\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  # Get all JPEG image file paths in path_images and sort them\n",
        "  jpg_files = sorted(\n",
        "      [\n",
        "          os.path.join(path_data, file_name)\n",
        "          for file_name in os.listdir(path_data)\n",
        "          if file_name.endswith(\".jpg\")\n",
        "      ]\n",
        "  )\n",
        "  return xml_files, jpg_files\n",
        "\n",
        "\n",
        "path_data_train = os.path.join(data_path, \"train\")\n",
        "path_data_valid = os.path.join(data_path, \"valid\")\n",
        "\n",
        "xml_files_train, jpg_files_train = parse_directory(path_data_train)\n",
        "xml_files_valid, jpg_files_valid = parse_directory(path_data_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yZe-Kxc1HA5"
      },
      "source": [
        "Nombre d'image du dataset :\\\n",
        "roboflow annonce 1251"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dUA06eZ0K9dz"
      },
      "outputs": [],
      "source": [
        "len(xml_files_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WznuOeVqLCkE"
      },
      "outputs": [],
      "source": [
        "len(jpg_files_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBDsHwyNxYsB"
      },
      "source": [
        "check the annotation files for one image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Er1rvbdayZ_F"
      },
      "outputs": [],
      "source": [
        "# l'image\n",
        "# filepath1 = \"/content/head-detection-1/train/2015_05_08_07_54_19FrontColor-aviImagen6_jpg.rf.70e018b14f081a5fbc5313cd11f24e3b.jpg\"\n",
        "# image1 = keras.utils.load_img(filepath1)\n",
        "\n",
        "# l'annotation\n",
        "# xml_file1 = \"/content/head-detection-1/train/2015_05_08_07_54_19FrontColor-aviImagen6_jpg.rf.70e018b14f081a5fbc5313cd11f24e3b.xml\"\n",
        "# tree1 = ET.parse(xml_file1)\n",
        "# root1 = tree1.getroot()\n",
        "# xml_str1 = ET.tostring(root1, encoding='unicode')\n",
        "# print(\"filename : \", root1.find(\"filename\").text)\n",
        "# print(\"xml_str : \", xml_str1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QpLtlbBRsvB"
      },
      "source": [
        "cree:\n",
        "```\n",
        "bounding_boxes = {\n",
        "    # num_boxes may be a Ragged dimension\n",
        "    'boxes': Tensor(shape=[batch, num_boxes, 4]),\n",
        "    'classes': Tensor(shape=[batch, num_boxes])\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35foj_ajLdcV"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(path_data, xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    image_name = root.find(\"filename\").text\n",
        "    image_path = os.path.join(path_data, image_name)\n",
        "\n",
        "    boxes = []\n",
        "    classes = []\n",
        "    for obj in root.iter(\"object\"):\n",
        "        cls = obj.find(\"name\").text\n",
        "        classes.append(cls)\n",
        "\n",
        "        bbox = obj.find(\"bndbox\")\n",
        "        xmin = float(bbox.find(\"xmin\").text)\n",
        "        ymin = float(bbox.find(\"ymin\").text)\n",
        "        xmax = float(bbox.find(\"xmax\").text)\n",
        "        ymax = float(bbox.find(\"ymax\").text)\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "    class_ids = [\n",
        "        list(class_mapping.keys())[list(class_mapping.values()).index(cls)]\n",
        "        for cls in classes\n",
        "    ]\n",
        "    return image_path, boxes, class_ids\n",
        "\n",
        "def create_bounding_boxes(path_data, xml_files):\n",
        "  image_paths = []\n",
        "  bbox = []\n",
        "  classes = []\n",
        "  for xml_file in tqdm(xml_files):\n",
        "      image_path, boxes, class_ids = parse_annotation(path_data, xml_file)\n",
        "      image_paths.append(image_path)\n",
        "      bbox.append(boxes)\n",
        "      classes.append(class_ids)\n",
        "\n",
        "  # create irregular tensor with tf.ragged\n",
        "  bbox = tf.ragged.constant(bbox)\n",
        "  classes = tf.ragged.constant(classes)\n",
        "  image_paths = tf.ragged.constant(image_paths)\n",
        "\n",
        "  return image_paths, bbox, classes\n",
        "\n",
        "\n",
        "image_paths_train, bbox_train, classes_train = create_bounding_boxes(path_data_train, xml_files_train)\n",
        "image_paths_valid, bbox_valid, classes_valid = create_bounding_boxes(path_data_valid, xml_files_valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ev5CtD5BLgm1"
      },
      "outputs": [],
      "source": [
        "# classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oaRLPbRLmgF"
      },
      "outputs": [],
      "source": [
        "# create the dataset\n",
        "data_train = tf.data.Dataset.from_tensor_slices((image_paths_train, classes_train, bbox_train))\n",
        "data_valid = tf.data.Dataset.from_tensor_slices((image_paths_valid, classes_valid, bbox_valid))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do_cJGf-LxvT"
      },
      "source": [
        "## Splitting data in training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zviPKC7PLuye"
      },
      "outputs": [],
      "source": [
        "# plus de splitting, on se base sur le splitting du dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGyOIWRz1f3M"
      },
      "source": [
        "Train des dataset train_data et val_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bip7k5eK1qSm"
      },
      "outputs": [],
      "source": [
        "print (f\"Nombre d'élément du dataset train_data : {len(data_train)}\")\n",
        "print (f\"Nombre d'élément de dataset val_data : {len(data_valid)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGWQBgDZL9mg"
      },
      "source": [
        "Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGFvW422L5EU"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_dataset(image_path, classes, bbox):\n",
        "    # Read Image\n",
        "    image = load_image(image_path)\n",
        "    bounding_boxes = {\n",
        "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
        "        \"boxes\": bbox,\n",
        "    }\n",
        "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZhyLkGkL_-R"
      },
      "outputs": [],
      "source": [
        "augmenter = keras.Sequential(\n",
        "    layers=[\n",
        "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n",
        "        keras_cv.layers.RandomShear(\n",
        "            x_factor=0.2, y_factor=0.2, bounding_box_format=\"xyxy\"\n",
        "        ),\n",
        "        keras_cv.layers.JitteredResize(\n",
        "            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xyxy\"\n",
        "        ),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rYslCW6MD6y"
      },
      "outputs": [],
      "source": [
        "resizing = keras_cv.layers.JitteredResize(\n",
        "    target_size=(640, 640),\n",
        "    scale_factor=(0.75, 1.3),\n",
        "    bounding_box_format=\"xyxy\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyEoUY-UMM-3"
      },
      "source": [
        "Creating Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjBLLJfOML_b"
      },
      "outputs": [],
      "source": [
        "train_ds = data_train.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(BATCH_SIZE * 4)\n",
        "train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tchfP-uMbAu"
      },
      "source": [
        "Creating Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksvRqq7tMdaD"
      },
      "outputs": [],
      "source": [
        "val_ds = data_valid.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.shuffle(BATCH_SIZE * 4)\n",
        "val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZrH_3n7Mg9o"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9gpg56SMh6X"
      },
      "outputs": [],
      "source": [
        "BOUNDING_BOX_FORMAT = \"xyxy\"\n",
        "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
        "    inputs = next(iter(inputs.take(1)))\n",
        "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
        "    visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=value_range,\n",
        "        rows=rows,\n",
        "        cols=cols,\n",
        "        y_true=bounding_boxes,\n",
        "        scale=5,\n",
        "        font_scale=0.7,\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        class_mapping=class_mapping,\n",
        "    )\n",
        "\n",
        "visualize_dataset(\n",
        "    train_ds, bounding_box_format=BOUNDING_BOX_FORMAT, value_range=(0, 255), rows=2, cols=2\n",
        ")\n",
        "\n",
        "visualize_dataset(\n",
        "    val_ds, bounding_box_format=BOUNDING_BOX_FORMAT, value_range=(0, 255), rows=2, cols=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syDnDwFQNBTL"
      },
      "source": [
        "We need to extract the inputs from the preprocessing dictionary and get them ready to be fed into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uexubj5NCiU"
      },
      "outputs": [],
      "source": [
        "def dict_to_tuple(inputs):\n",
        "    #return inputs[\"images\"], inputs[\"bounding_boxes\"]  # init\n",
        "    return inputs[\"images\"], bounding_box.to_dense(\n",
        "        inputs[\"bounding_boxes\"], max_boxes=32\n",
        "    )\n",
        "\n",
        "\n",
        "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t30nAjPINJUH"
      },
      "outputs": [],
      "source": [
        "train_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Nxeuhacc3X-"
      },
      "source": [
        "# Entrainement par transfert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK3zDAwENOlg"
      },
      "source": [
        "List des models YoloV8 :  https://keras.io/api/keras_cv/models/backbones/yolo_v8/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bu6T1bLdKD2"
      },
      "source": [
        "##Téléchargement d'un model YoloV8 pre-entrainé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cw0Y1xvOTzA"
      },
      "outputs": [],
      "source": [
        "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
        "    YOLOV8BACKBONE  # see YOLOV8BACKBONE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WaEW4ggOoGy"
      },
      "source": [
        "integration de nos paramètres :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYuBGLOQOnvo"
      },
      "outputs": [],
      "source": [
        "yolo = keras_cv.models.YOLOV8Detector(\n",
        "    num_classes=len(class_mapping),\n",
        "    bounding_box_format=BOUNDING_BOX_FORMAT,\n",
        "    backbone=backbone,\n",
        "    fpn_depth=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_Aikc0_Ov6T"
      },
      "source": [
        "Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JDNz_Z9OwoT"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    global_clipnorm=GLOBAL_CLIPNORM,\n",
        ")\n",
        "\n",
        "yolo.compile(\n",
        "    optimizer=optimizer,\n",
        "    classification_loss=\"binary_crossentropy\",\n",
        "    box_loss=\"ciou\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z73YMHxnO29j"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHb8hFZQT6fV"
      },
      "outputs": [],
      "source": [
        "# start a new wandb run to track this script\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"Head détection V1.5 YOLO_S EPOCH\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"global_clipnorm\" : GLOBAL_CLIPNORM,\n",
        "    \"YOLOV8Backbone\": YOLOV8BACKBONE,\n",
        "    \"dataset\": \"https://universe.roboflow.com/head-znpny/head-detection-nej1a/dataset/1\",\n",
        "    \"epochs\": EPOCH,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"num_classes\": len(class_mapping),\n",
        "    \"bounding_box_format\": BOUNDING_BOX_FORMAT,\n",
        "    \"augmentation\": \"Yes\",\n",
        "    \"early_stopping\": \"No\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwuHuu2JtY45"
      },
      "outputs": [],
      "source": [
        "coco_metrics_callback = keras_cv.callbacks.PyCOCOCallback(\n",
        "    val_ds.take(20), bounding_box_format=BOUNDING_BOX_FORMAT\n",
        ")\n",
        "\n",
        "# https://keras.io/api/callbacks/early_stopping/\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"box_loss\",\n",
        "    min_delta=1e-4,\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=20,\n",
        ")\n",
        "\n",
        "'''\n",
        "# https://keras.io/api/callbacks/model_checkpoint/\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=config[\"checkpoint_path\"],\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbPDdrrtO3nl"
      },
      "outputs": [],
      "source": [
        "print(\"Fit model on training data\")\n",
        "yolo.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCH,\n",
        "    callbacks=[\n",
        "      WandbMetricsLogger(),\n",
        "      WandbModelCheckpoint(\"models\"),\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "# coco_metrics_callback\n",
        "# early_stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5cAYbhtSvN5"
      },
      "outputs": [],
      "source": [
        "# [optional] finish the wandb run, necessary in notebooks\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJrxJgzwuobf"
      },
      "outputs": [],
      "source": [
        "# The returned history object holds a record of the loss values and metric values during training:\n",
        "print(yolo.history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU4SCvvY1vEN"
      },
      "outputs": [],
      "source": [
        "def plot_hist(hist):\n",
        "    plt.plot(hist.history[\"loss\"])\n",
        "    plt.plot(hist.history[\"box_loss\"])\n",
        "    plt.title(\"model box_loss\")\n",
        "    plt.ylabel(\"box_loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n",
        "plot_hist(yolo.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wecjj0bEVSGX"
      },
      "source": [
        "## Sauvegarde du model après entrainement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6lJWVVPVSvD"
      },
      "outputs": [],
      "source": [
        "# yolo.save(\"myYolo_HeadDetect_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9JQETLfVr1O"
      },
      "source": [
        "# Utilisation de notre model entrainé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnfl-i-2epJh"
      },
      "source": [
        "##Load de notre model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nht_5X2eeu3-"
      },
      "outputs": [],
      "source": [
        "# pour être plus rapide à l'execution, je ne sauve pas le modele, donc je ne le recharge pas\n",
        "'''\n",
        "# need to specify the bounding_box_format\n",
        "BOUNDING_BOX_FORMAT = \"xyxy\"\n",
        "custom_objects = {'CIoULoss': keras_cv.losses.CIoULoss}\n",
        "\n",
        "my_model = keras.models.load_model(\"myYolo_HeadDetect_model\",\n",
        "                                   custom_objects=custom_objects,\n",
        "                                   compile=False)\n",
        "\n",
        "# Compile the model after loading\n",
        "my_model.compile(optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybmun1nifTgi"
      },
      "source": [
        "## Utilisation du model avec une image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-74bPhtIVvbF"
      },
      "source": [
        "On prend une image du dataset\n",
        "2015_05_08_08_00_40FrontColor-aviImagen3_jpg.rf.13e2ef1b695dc8d63e355a7a205d1ab9.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbAK27OXVyot"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "filepath = \"/content/head-detection-1/test/2015_05_10_09_43_50FrontColor-aviImagen1_jpg.rf.c6bdcbce984aced0f03031e996121441.jpg\"\n",
        "image = keras.utils.load_img(filepath)\n",
        "image = np.array(image)\n",
        "\n",
        "visualization.plot_image_gallery(\n",
        "    np.array([image]),\n",
        "    value_range=(0, 255),\n",
        "    rows=1,\n",
        "    cols=1,\n",
        "    scale=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dW3KHI2WKkj"
      },
      "source": [
        "## Perform detections with my model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75GFIut6WLLR"
      },
      "outputs": [],
      "source": [
        "# redimensionnement de l'image avec ratio identique\n",
        "inference_resizing = keras_cv.layers.Resizing(\n",
        "    640, 640, pad_to_aspect_ratio=True, bounding_box_format=BOUNDING_BOX_FORMAT\n",
        ")\n",
        "\n",
        "image_batch = inference_resizing([image])\n",
        "len(image_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ANAFNyIfWQyp"
      },
      "outputs": [],
      "source": [
        "# avec notre model en direct\n",
        "my_model = yolo\n",
        "\n",
        "y_pred1 = my_model.predict(image_batch)\n",
        "y_pred1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMDbXn2wWVXZ"
      },
      "outputs": [],
      "source": [
        "# visualizase :\n",
        "visualization.plot_bounding_box_gallery(\n",
        "    image_batch,\n",
        "    value_range=(0, 255),\n",
        "    rows=1,\n",
        "    cols=1,\n",
        "    y_pred=y_pred1,\n",
        "    scale=5,\n",
        "    font_scale=0.7,\n",
        "    bounding_box_format=BOUNDING_BOX_FORMAT,\n",
        "    class_mapping=class_mapping,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21EzElAcWdjm"
      },
      "source": [
        "On trouve une tête, marche pas a tout les coups\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRltEeHajhav"
      },
      "source": [
        "## Validation du model pré-entrainé avec le dataset Test : test_on_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r0ds3NZ9QAp"
      },
      "source": [
        "### load du dataset de Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jd3GYixjz_W"
      },
      "outputs": [],
      "source": [
        "# load du dataset de Test\n",
        "\n",
        "# parse file from Test folder\n",
        "path_annot = \"head-detection-1/test\"\n",
        "path_images  = \"head-detection-1/test\"\n",
        "\n",
        "# Get all XML file paths in path_annot and sort them\n",
        "xml_files_T = sorted(\n",
        "    [\n",
        "        os.path.join(path_annot, file_name)\n",
        "        for file_name in os.listdir(path_annot)\n",
        "        if file_name.endswith(\".xml\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Get all JPEG image file paths in path_images and sort them\n",
        "jpg_files_T = sorted(\n",
        "    [\n",
        "        os.path.join(path_images, file_name)\n",
        "        for file_name in os.listdir(path_images)\n",
        "        if file_name.endswith(\".jpg\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "image_paths_T = []\n",
        "bbox_T = []\n",
        "classes_T = []\n",
        "for xml_file in tqdm(xml_files_T):\n",
        "    image_path_T, boxes_T, class_ids_T = parse_annotation(path_annot, xml_file)\n",
        "    image_paths_T.append(image_path_T)\n",
        "    bbox_T.append(boxes_T)\n",
        "    classes_T.append(class_ids_T)\n",
        "\n",
        "\n",
        "\n",
        "# create irregular tensor with tf.ragged\n",
        "classes_T = tf.ragged.constant(classes_T)\n",
        "bbox_T = tf.ragged.constant(bbox_T)\n",
        "image_paths_T = tf.ragged.constant(image_paths_T)\n",
        "\n",
        "# create the dataset\n",
        "data_T = tf.data.Dataset.from_tensor_slices((image_paths_T, classes_T, bbox_T))\n",
        "\n",
        "Test_ds = data_T.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "Test_ds = Test_ds.shuffle(BATCH_SIZE * 4)\n",
        "Test_ds = Test_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
        "Test_ds = Test_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "# visualize Test_ds\n",
        "visualize_dataset(\n",
        "    Test_ds, bounding_box_format=BOUNDING_BOX_FORMAT, value_range=(0, 255), rows=2, cols=2\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUrVFTZV9yfZ"
      },
      "source": [
        "### Test_on_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPj61KJxEAoA"
      },
      "outputs": [],
      "source": [
        "''' desactivation pour le moment\n",
        "\n",
        "# Load and preprocess the images\n",
        "image_batch = []\n",
        "input_height = 640\n",
        "input_width = 640\n",
        "for image_path in image_paths_T:\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    # Resize image if necessary to match your model's input size\n",
        "    image = tf.image.resize(image, (input_height, input_width))\n",
        "    image_batch.append(image)\n",
        "\n",
        "image_batch = tf.stack(image_batch) # Stack images into a batch tensor\n",
        "\n",
        "# Prepare labels and bounding boxes in the expected format (dictionary)\n",
        "y_true = {\n",
        "    \"classes\": classes_T.to_tensor(),  # Convert RaggedTensor to a regular tensor\n",
        "    \"boxes\": bbox_T.to_tensor()       # Convert RaggedTensor to a regular tensor\n",
        "}\n",
        "\n",
        "# Now test on the preprocessed image batch, passing the labels in the correct format\n",
        "my_model.test_on_batch(image_batch, y=y_true, sample_weight=None, return_dict=True)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UczHYAqGHDXF"
      },
      "source": [
        "A scalar loss value (when no metrics and return_dict=False), a list of loss and metric values (if there are metrics and return_dict=False), or a dict of metric and loss values (if return_dict=True)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCemM5QxHrmB"
      },
      "source": [
        "A comparer avec les stats de notre entrainement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0HAfaiucl_C"
      },
      "source": [
        "##Visualize Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip3IW7U4k-j-"
      },
      "source": [
        "Exemple dans https://keras.io/examples/vision/yolov8/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soUeO-kYctOg"
      },
      "outputs": [],
      "source": [
        "def visualize_detections(model, dataset, bounding_box_format):\n",
        "    images, y_true = next(iter(dataset.take(1)))\n",
        "    y_pred = model.predict(images)\n",
        "    y_pred = bounding_box.to_ragged(y_pred)\n",
        "    visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=(0, 255),\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        scale=4,\n",
        "        rows=2,\n",
        "        cols=2,\n",
        "        show=True,\n",
        "        font_scale=0.7,\n",
        "        class_mapping=class_mapping,\n",
        "    )\n",
        "\n",
        "'''\n",
        "yolo.prediction_decoder = keras_cv.layers.NonMaxSuppression(\n",
        "    from_logits=True,\n",
        "    bounding_box_format=BOUNDING_BOX_FORMAT,\n",
        "    iou_threshold=IOU_THRESHOLD,\n",
        "    confidence_threshold=0.55\n",
        ")\n",
        "'''\n",
        "#    from_logits=True,\n",
        "#    iou_threshold=IOU_THRESHOLD,\n",
        "\n",
        "\n",
        "visualize_detections(yolo, dataset=val_ds, bounding_box_format=\"xyxy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t26tl1dbjmt0"
      },
      "source": [
        "## Intersection over Union (IoU) for object detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPuwFxoSm7-P"
      },
      "source": [
        "https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\\\n",
        "\n",
        "Intersection over Union (IoU) is used to evaluate the performance of object detection by comparing the ground truth bounding box to the preddicted bounding box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmyjgRbUkTRl"
      },
      "outputs": [],
      "source": [
        "# DEBUG\n",
        "Test_ds # dataset avec image et boxes resize\n",
        "\n",
        "'''\n",
        "<_ParallelMapDataset\n",
        "element_spec={\n",
        "    'images': TensorSpec(shape=(4, 640, 640, 3), dtype=tf.float32, name=None),\n",
        "    'bounding_boxes': {\n",
        "        'classes': RaggedTensorSpec(TensorShape([4, None]), tf.float32, 1, tf.int64),\n",
        "        'boxes': RaggedTensorSpec(TensorShape([4, None, None]), tf.float32, 1, tf.int64)\n",
        "    }\n",
        "}>\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKv5foByn4J2"
      },
      "outputs": [],
      "source": [
        "# method pour la compile pour reduire le IoU\n",
        "'''\n",
        "tf.keras.metrics.MeanIoU(\n",
        "    num_classes,\n",
        "    name=None,\n",
        "    dtype=None,\n",
        "    ignore_class=None,\n",
        "    sparse_y_true=True,\n",
        "    sparse_y_pred=True,\n",
        "    axis=-1\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgE4mR-MpEyt"
      },
      "outputs": [],
      "source": [
        "''' desactivation pour le moment\n",
        "\n",
        "def visualize_detections_and_compute_iou(model, dataset, bounding_box_format):\n",
        "    images, y_true = next(iter(dataset.take(1)))\n",
        "    print(f\"Nombre d'y_true: {len(y_true)}\")\n",
        "    y_pred = model.predict(images)\n",
        "    #y_pred = bounding_box.to_ragged(y_pred)\n",
        "    visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=(0, 255),\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        scale=4,\n",
        "        rows=2,\n",
        "        cols=2,\n",
        "        show=True,\n",
        "        font_scale=0.7,\n",
        "        class_mapping=class_mapping,\n",
        "    )\n",
        "\n",
        "    # No conversion needed, they are already dense tensors\n",
        "    y_true_boxes_dense = y_true['boxes']\n",
        "    y_pred_boxes_dense = y_pred['boxes']\n",
        "\n",
        "\n",
        "    # pour calculer le IoU\n",
        "    # boxes1 : les boxes du vals_ds\n",
        "    # boxes2 : les boxes calculées du val_ds\n",
        "    iou = keras_cv.bounding_box.compute_iou(\n",
        "        boxes1 = y_true_boxes_dense,\n",
        "        boxes2 = y_pred_boxes_dense,\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        use_masking=False,\n",
        "        mask_val=-1,\n",
        "        images=None,\n",
        "        image_shape=None,\n",
        "    )\n",
        "    return iou\n",
        "\n",
        "\n",
        "iou = visualize_detections_and_compute_iou(yolo, dataset=val_ds, bounding_box_format=\"xyxy\")\n",
        "print(iou)\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNaoH2HaLWZk/KgyKSIEwdH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}